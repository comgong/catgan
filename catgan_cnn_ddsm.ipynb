{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "mb_size = 48\n",
    "batch_size = 48\n",
    "X_dim = [224,224,3]\n",
    "z_dim = 300\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "d_steps = 3\n",
    "n_class = 2\n",
    "nz = 300 # z dim\n",
    "cross_entropy_term = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN FUNCTION LOAD DDSM DATA\n",
      "YO YO YO\n",
      "LEN OF MASK DIR 1592\n",
      "loading in 1108 images...\n",
      "loading in 123 images...\n",
      "loading in 123 images...\n",
      "(1104, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from dataset import load_ddsm_data\n",
    "ddsm_path = '/dfs/scratch0/annhe/tanda_750_90_10_split/'\n",
    "labels_fl = 'mass_to_label.json'\n",
    "labels_path = os.path.join(ddsm_path,labels_fl)\n",
    "\n",
    "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = load_ddsm_data(data_dir=ddsm_path, \\\n",
    "    label_json=ddsm_path+'/'+'mass_to_label.json', validation_set=True, segmentations=False, as_float=True, channels=3)\n",
    "X_train = X_train[0:1104,:]\n",
    "Y_train = Y_train[0:1104,:]\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "# x_train = mnist.train.images[:50,:]\n",
    "# x_train = x_train.reshape([50,28,28,1])\n",
    "# #randomNum = random.randint(0,25)\n",
    "# image = x_train[0]\n",
    "# plt.imshow(image[:,:,0], cmap=plt.get_cmap('gray_r'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    #plt.figure(figsize=(800/my_dpi, 800/my_dpi), dpi=my_dpi)\n",
    "    fig = plt.figure(figsize=(200, 200))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    #print len(samples)\n",
    "    for i, sample in enumerate(samples):\n",
    "        #print i\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        sample_one_layer = sample[:,:,0]\n",
    "        #print sample_one_layer.shape\n",
    "        plt.imshow(sample_one_layer.reshape(224, 224), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    return tf.log(x + 1e-8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\n",
    "            name='image', dtype=tf.float32,\n",
    "            shape=[batch_size, 224, 224, 3],\n",
    "        )\n",
    "z = tf.placeholder(tf.float32, shape=[None, nz])\n",
    "y = tf.placeholder(\n",
    "            name='label', dtype=tf.float32, shape=[batch_size, n_class],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = np.array([224, 224, 2, 3])\n",
    "\n",
    "conv_info = np.array([64, 128, 256])\n",
    "\n",
    "deconv_info = np.array([[300, 3, 1], [100, 7, 2], [50, 5, 2], [25, 5, 2], [12, 6, 2], [3, 6, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "def huber_loss(labels, predictions, delta=1.0):\n",
    "    residual = tf.abs(predictions - labels)\n",
    "    condition = tf.less(residual, delta)\n",
    "    small_res = 0.5 * tf.square(residual)\n",
    "    large_res = delta * residual - 0.5 * tf.square(delta)\n",
    "    return tf.where(condition, small_res, large_res)\n",
    "\n",
    "def conv2d(input, output_shape, is_train, k_h=5, k_w=5, stddev=0.02, name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input.get_shape()[-1], output_shape],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape], initializer=tf.constant_initializer(0.0))\n",
    "        conv = lrelu(tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape()))\n",
    "        bn = tf.contrib.layers.batch_norm(conv, center=True, scale=True,\n",
    "                                          decay=0.9, is_training=is_train,\n",
    "                                          updates_collections=None)\n",
    "    return bn\n",
    "\n",
    "\n",
    "def deconv2d(input, deconv_info, is_train, name=\"deconv2d\", stddev=0.02, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        output_shape = deconv_info[0]\n",
    "        k = deconv_info[1]\n",
    "        s = deconv_info[2]\n",
    "        deconv = layers.conv2d_transpose(\n",
    "            input, num_outputs=output_shape,\n",
    "            weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "            biases_initializer=tf.zeros_initializer(),\n",
    "            kernel_size=[k, k], stride=[s, s], padding='VALID'\n",
    "        )\n",
    "        if not activation_fn:\n",
    "            deconv = tf.nn.relu(deconv)\n",
    "            deconv = tf.contrib.layers.batch_norm(\n",
    "                deconv, center=True, scale=True,  decay=0.9,\n",
    "                is_training=is_train, updates_collections=None\n",
    "            )\n",
    "        else:\n",
    "            deconv = activation_fn(deconv)\n",
    "        return deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train=True\n",
    "image_shape = [batch_size, 224, 224, 3]\n",
    "def G(z, scope='Generator'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        #log.warn(scope.name)\n",
    "        #print z.shape\n",
    "        z = tf.reshape(z, [batch_size, 1, 1, -1])\n",
    "        #print z.shape\n",
    "        g_1 = deconv2d(z, deconv_info[0], is_train, name='g_1_deconv')\n",
    "        #print g_1.shape\n",
    "        #log.info('{} {}'.format(scope.name, g_1))\n",
    "        g_2 = deconv2d(g_1, deconv_info[1], is_train, name='g_2_deconv')\n",
    "        #print g_2.shape\n",
    "        #log.info('{} {}'.format(scope.name, g_2))\n",
    "        g_3 = deconv2d(g_2, deconv_info[2], is_train, name='g_3_deconv')\n",
    "        #print g_3.shape\n",
    "        #log.info('{} {}'.format(scope.name, g_3))\n",
    "        g_4 = deconv2d(g_3, deconv_info[3], is_train, name='g_4_deconv', activation_fn=tf.tanh)\n",
    "        #print g_4.shape\n",
    "        g_5 = deconv2d(g_4, deconv_info[4], is_train, name='g_5_deconv', activation_fn=tf.tanh)\n",
    "        #print g_5.shape\n",
    "        g_6 = deconv2d(g_5, deconv_info[5], is_train, name='g_6_deconv', activation_fn=tf.tanh)\n",
    "        #print g_6.shape\n",
    "        #log.info('{} {}'.format(scope.name, g_4))\n",
    "        output = g_6\n",
    "        #print X.get_shape().as_list()\n",
    "        assert output.get_shape().as_list() == image_shape, output.get_shape().as_list()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D(img, scope='Discriminator', reuse=True):\n",
    "    with tf.variable_scope(scope, reuse=reuse) as scope:\n",
    "        #if not reuse: log.warn(scope.name)\n",
    "        #print img.shape\n",
    "        d_1 = conv2d(img, conv_info[0], is_train, name='d_1_conv')\n",
    "        d_1 = slim.dropout(d_1, keep_prob=0.5, is_training=is_train, scope='d_1_conv/')\n",
    "        #print d_1.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_1))\n",
    "        d_2 = conv2d(d_1, conv_info[1], is_train, name='d_2_conv')\n",
    "        d_2 = slim.dropout(d_2, keep_prob=0.5, is_training=is_train, scope='d_2_conv/')\n",
    "        #print d_2.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_2))\n",
    "        d_3 = conv2d(d_2, conv_info[2], is_train, name='d_3_conv')\n",
    "        d_3 = slim.dropout(d_3, keep_prob=0.5, is_training=is_train, scope='d_3_conv/')\n",
    "        #print d_3.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_3))\n",
    "        d_4 = slim.fully_connected(\n",
    "            tf.reshape(d_3, [batch_size, -1]), n_class, scope='d_4_fc', activation_fn=None)\n",
    "        #print d_4.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_4))\n",
    "        output = d_4\n",
    "        assert output.get_shape().as_list() == [batch_size, n_class]\n",
    "        return tf.nn.softmax(output), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0e-6\n",
    "LAMBA = 1\n",
    "# y has dim batch_size x num_classes\n",
    "# entropy 1\n",
    "def marginal_entropy(y):\n",
    "    y_1 = tf.reduce_mean(y, axis=0) #1/N sum y_i\n",
    "    y_2 = -y_1 * tf.log(y_1+epsilon)\n",
    "    y_3 = tf.reduce_sum(y_2)\n",
    "    return y_3\n",
    "\n",
    "def entropy(y):\n",
    "    #batch_size= K.int_shape(y)[0]\n",
    "    y_1 = -y * tf.log(y+epsilon)\n",
    "    y_2 = tf.reduce_sum(y_1,axis=1)\n",
    "    y_3 = tf.reduce_mean(y_2,axis=0)\n",
    "    return y_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300   3   1]\n"
     ]
    }
   ],
   "source": [
    "print deconv_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list of np arrays\n",
    "def average_grads(grads):\n",
    "    #print type(grads)\n",
    "    #print len(grads)\n",
    "    #print type(grads[0])\n",
    "    #print len(grads[0])\n",
    "    #print type(grads[0][0])\n",
    "    grads_list = []\n",
    "    for grad in grads:\n",
    "        grads_list.append(tf.reduce_mean(grad[0]))\n",
    "    #print \"from average grads\"\n",
    "    #print type(grads_list[0])\n",
    "    return grads_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from util import log\n",
    "#z = tf.random_uniform([batch_size, nz], minval=-1, maxval=1, dtype=tf.float32)\n",
    "z = tf.random_normal([batch_size, nz], dtype=tf.float32)\n",
    "G_sample = G(z)\n",
    "\n",
    "D_real, D_real_logits = D(X, scope='Discriminator', reuse=False)\n",
    "D_fake, D_fake_logits = D(G_sample, scope='Discriminator', reuse=True)\n",
    "\n",
    "D_target = 1./mb_size\n",
    "G_target = 1./(mb_size*2)\n",
    "\n",
    "#Z = tf.reduce_sum(tf.exp(-D_real)) + tf.reduce_sum(tf.exp(-D_fake))\n",
    "\n",
    "#D_loss = tf.reduce_sum(D_target * D_real) + log(Z)\n",
    "#G_loss = tf.reduce_sum(G_target * D_real) + tf.reduce_sum(G_target * D_fake) + log(Z)\n",
    "\n",
    "D_loss = -marginal_entropy(D_real) + entropy(D_real) - entropy(D_fake) + cross_entropy_term *tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=D_real_logits))\n",
    "G_loss = -marginal_entropy(D_fake) + entropy(D_fake)\n",
    "\n",
    "all_vars = tf.trainable_variables()\n",
    "\n",
    "theta_D = [v for v in all_vars if v.name.startswith('Discriminator')]\n",
    "#log.warn(\"********* d_var ********** \"); slim.model_analyzer.analyze_vars(d_var, print_info=True)\n",
    "\n",
    "theta_G = [v for v in all_vars if v.name.startswith(('Generator'))]\n",
    "#log.warn(\"********* g_var ********** \"); slim.model_analyzer.analyze_vars(g_var, print_info=True)\n",
    "\n",
    "D_grad_overall = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .compute_gradients(D_loss, var_list=theta_D, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N))\n",
    "#D_grad = average_grads(D_grad_overall)\n",
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "G_grad_overall = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "             .compute_gradients(G_loss, var_list=theta_G, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N))\n",
    "#G_grad = average_grads(G_grad_overall)\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=theta_G))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "\n",
    "dir_name ='out_15/'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; D_loss: 0.2627; G_loss: -0.1212\n",
      "[0.028256232, -0.23190486, 0.045735359, 0.00014140853, -8.4835032e-05, 0.027549952, -0.0046366984, 4.8778951e-05, 0.00071822293, 0.0013668451, -0.034829512, 0.015196411, 6.0821065e-10, -1.4901161e-08]\n",
      "Iter: 10; D_loss: 1.991; G_loss: -0.5053\n",
      "[0.0028884388, -0.040382765, 0.0065481514, 0.00015451864, -0.00048534453, -0.001294991, -0.0049155233, 8.6356886e-06, -0.00040806844, 0.0011629176, 0.01345334, 0.0046254108, -3.8013168e-10, -3.7252903e-09]\n",
      "Iter: 20; D_loss: 1.006; G_loss: -0.2817\n",
      "[0.012913963, -0.0063682273, 0.040414196, 0.00032309024, -0.00044842408, -0.0053592334, 0.0037295159, 1.0071322e-05, 0.00088900817, 0.00042993523, 0.0019047812, 0.0094862264, 8.3628965e-10, 1.0244548e-08]\n",
      "Iter: 0; D_loss: 0.6205; G_loss: -0.4489\n",
      "[-0.0079223355, -0.26771981, 0.031059703, -2.8874259e-05, -0.00062578177, -0.0078999875, 0.0001919957, 1.30306e-05, 0.00046275667, -0.00037026231, 0.0011047984, 0.0073662717, 0.0, 1.8626451e-09]\n",
      "Iter: 10; D_loss: 1.703; G_loss: -0.3582\n",
      "[0.001060646, -0.03834305, 0.013040346, -0.00039356889, 2.796336e-05, -0.0069097253, 0.0054768119, 1.8894207e-05, -0.00030933216, -2.4747904e-05, 0.0005366347, 0.010960123, -3.0410532e-10, 9.3132257e-09]\n",
      "Iter: 20; D_loss: 2.219; G_loss: -0.364\n",
      "[-0.0061873794, 0.027450737, 0.0085434113, 0.00021107122, 0.00041791264, 2.1821994e-05, 0.00027617803, 1.9264946e-05, 0.00020692003, 0.00040569092, 0.001158122, 0.0076288874, -3.0410532e-10, -6.519258e-09]\n",
      "Iter: 0; D_loss: 1.856; G_loss: -0.1429\n",
      "[0.014457275, -0.0090869181, -0.0077386117, 0.0001100878, -0.00052262994, 0.0045408178, -4.2088912e-05, 8.9433743e-06, -0.0010035507, 0.00075585372, -2.6587804e-06, 0.014009314, 1.824632e-09, -5.5879354e-09]\n",
      "Iter: 10; D_loss: 2.837; G_loss: -0.5526\n",
      "[0.00059622707, -0.0036988705, -0.0036791805, 7.5278804e-05, -0.00050371583, 0.0067785792, 0.004527031, 4.1372026e-05, -0.00021160136, 0.001376065, -0.00068331766, 0.015404949, -1.824632e-09, 3.7252903e-09]\n",
      "Iter: 20; D_loss: 5.111; G_loss: -0.4427\n",
      "[-0.00034972528, -0.084011063, -0.0043549556, 6.7063782e-05, -0.0010625368, 0.0008647283, 0.00043100747, 4.8068818e-05, 0.00022116151, 0.000616815, -0.002103331, 0.016376827, 2.28079e-10, 3.7252903e-09]\n",
      "Iter: 0; D_loss: 3.93; G_loss: -0.4647\n",
      "[-0.0018065842, -0.004261211, -0.004866947, -8.107163e-07, -0.00061996677, 0.0071435617, -0.0061072716, 3.1672418e-05, 0.00033439271, 0.00039001461, 0.00078866055, 0.010759873, -1.2164213e-09, -1.2340024e-08]\n",
      "Iter: 10; D_loss: 4.956; G_loss: -0.5793\n",
      "[0.0031574238, -0.037840284, 0.0035502622, 2.3842324e-05, -0.00015332751, -0.00236852, -0.002659275, 2.1590997e-05, 7.6820515e-06, -0.00012808255, -0.0018817682, 0.0069140191, -4.3715143e-10, -1.8626451e-09]\n",
      "Iter: 20; D_loss: 4.977; G_loss: -0.5893\n",
      "[-0.0018427426, -0.033276618, -0.0097569469, -2.7352071e-05, -0.00078222662, 0.0019794572, 0.0021025513, 1.5510886e-07, 0.00021405872, 0.00010680669, 0.0025533927, 0.016065549, -1.2164213e-09, 1.8626451e-09]\n",
      "Iter: 0; D_loss: 3.799; G_loss: -0.6596\n",
      "[-0.003428696, -0.043740928, -0.0023017032, -1.6259393e-05, -0.00017439749, 0.0010967072, 0.0016922911, 4.7801324e-05, -9.8857308e-05, 0.00073865167, -0.00029094447, 0.0097247949, -1.2164213e-09, 7.4505806e-09]\n"
     ]
    }
   ],
   "source": [
    "#100,000 is good\n",
    "#print every 1000\n",
    "g_loss_overall = []\n",
    "d_loss_overall = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    N_train = X_train.shape[0]\n",
    "    n_batches = int(np.ceil(N_train / float(batch_size)))\n",
    "    for it in range(num_epochs):\n",
    "        for i, b in enumerate(range(0, N_train, batch_size)):\n",
    "        #for i in range(1):\n",
    "            X_batch = X_train[b : b + batch_size, :]\n",
    "            Y_batch = Y_train[b : b + batch_size, :]\n",
    "            if (X_batch.shape[0] != batch_size): continue\n",
    "#            X_batch = X_train[i : i + batch_size, :]\n",
    "#             Y_batch = Y_train[i : i + batch_size, :]\n",
    "\n",
    "            _, D_loss_curr = sess.run(\n",
    "                [D_solver, D_loss], feed_dict={X: X_batch, y: Y_batch}\n",
    "            )\n",
    "\n",
    "            _, G_loss_curr = sess.run(\n",
    "                [G_solver, G_loss], feed_dict={X: X_batch, y: Y_batch}\n",
    "            )\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "                  .format(i, D_loss_curr, G_loss_curr))\n",
    "                D_grad_cur_overall = sess.run(D_grad_overall, feed_dict={X: X_batch, y: Y_batch})\n",
    "                D_grad_curr = average_grads(D_grad_cur_overall)\n",
    "                D_grad_eval = []\n",
    "                for grad in D_grad_curr:\n",
    "                    D_grad_eval.append(grad.eval(session=sess))\n",
    "                print D_grad_eval\n",
    "                g_loss_overall.append(G_loss_curr)\n",
    "                d_loss_overall.append(D_loss_curr)\n",
    "                samples = sess.run(G_sample)\n",
    "                truncated_samples = samples[0:16,:]\n",
    "                fig = plot(truncated_samples)\n",
    "                plt.savefig(dir_name+'{}.png'\n",
    "                    .format(str(counter).zfill(3)), bbox_inches='tight')\n",
    "                counter += 1\n",
    "                plt.close(fig)\n",
    "        # Reshuffle\n",
    "        idxs = range(X_train.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        X_train = X_train[idxs]\n",
    "        Y_train = Y_train[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#print g_loss_overall\n",
    "#print d_loss_overall\n",
    "g_loss_plot, = plt.plot(g_loss_overall, label='G Loss')\n",
    "d_loss_plot, = plt.plot(d_loss_overall, label='D Loss')\n",
    "\n",
    "plt.legend([g_loss_plot, d_loss_plot], ['G Loss', 'D Loss'])\n",
    "plt.savefig(dir_name+'plot_for_catgan_.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

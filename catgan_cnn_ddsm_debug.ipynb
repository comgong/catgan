{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "mb_size = 5\n",
    "batch_size = 5\n",
    "X_dim = [224,224,3]\n",
    "z_dim = 500\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "d_steps = 3\n",
    "n_class = 2\n",
    "nz = 500 # z dim\n",
    "cross_entropy_term = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN FUNCTION LOAD DDSM DATA\n",
      "YO YO YO\n",
      "LEN OF MASK DIR 1592\n",
      "loading in 10 images...\n",
      "loading in 10 images...\n",
      "loading in 10 images...\n",
      "(10, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from dataset import load_ddsm_data\n",
    "ddsm_path = '/dfs/scratch0/annhe/tanda_750_90_10_split/'\n",
    "labels_fl = 'mass_to_label.json'\n",
    "labels_path = os.path.join(ddsm_path,labels_fl)\n",
    "\n",
    "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = load_ddsm_data(data_dir=ddsm_path, \\\n",
    "    label_json=ddsm_path+'/'+'mass_to_label.json', validation_set=True, segmentations=False, as_float=True, channels=3)\n",
    "X_train = X_train[0:1104,:]\n",
    "Y_train = Y_train[0:1104,:]\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "# x_train = mnist.train.images[:50,:]\n",
    "# x_train = x_train.reshape([50,28,28,1])\n",
    "# #randomNum = random.randint(0,25)\n",
    "# image = x_train[0]\n",
    "# plt.imshow(image[:,:,0], cmap=plt.get_cmap('gray_r'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    #print len(samples)\n",
    "    for i, sample in enumerate(samples):\n",
    "        #print i\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        sample_one_layer = sample[:,:,0]\n",
    "        #print sample_one_layer.shape\n",
    "        plt.imshow(sample_one_layer.reshape(224, 224), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    return tf.log(x + 1e-8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\n",
    "            name='image', dtype=tf.float32,\n",
    "            shape=[batch_size, 224, 224, 3],\n",
    "        )\n",
    "z = tf.placeholder(tf.float32, shape=[None, nz])\n",
    "y = tf.placeholder(\n",
    "            name='label', dtype=tf.float32, shape=[batch_size, n_class],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_info = np.array([224, 224, 2, 3])\n",
    "\n",
    "conv_info = np.array([64, 128, 256])\n",
    "\n",
    "deconv_info = np.array([[500, 6, 2], [100, 6, 2], [50, 6, 3], [25, 8, 2], [12, 8, 2], [3, 3, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "def huber_loss(labels, predictions, delta=1.0):\n",
    "    residual = tf.abs(predictions - labels)\n",
    "    condition = tf.less(residual, delta)\n",
    "    small_res = 0.5 * tf.square(residual)\n",
    "    large_res = delta * residual - 0.5 * tf.square(delta)\n",
    "    return tf.where(condition, small_res, large_res)\n",
    "\n",
    "def conv2d(input, output_shape, is_train, k_h=5, k_w=5, stddev=0.02, name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input.get_shape()[-1], output_shape],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape], initializer=tf.constant_initializer(0.0))\n",
    "        conv = lrelu(tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape()))\n",
    "        bn = tf.contrib.layers.batch_norm(conv, center=True, scale=True,\n",
    "                                          decay=0.9, is_training=is_train,\n",
    "                                          updates_collections=None)\n",
    "    return bn\n",
    "\n",
    "\n",
    "def deconv2d(input, deconv_info, is_train, name=\"deconv2d\", stddev=0.02, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        output_shape = deconv_info[0]\n",
    "        k = deconv_info[1]\n",
    "        s = deconv_info[2]\n",
    "        deconv = layers.conv2d_transpose(\n",
    "            input, num_outputs=output_shape,\n",
    "            weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "            biases_initializer=tf.zeros_initializer(),\n",
    "            kernel_size=[k, k], stride=[s, s], padding='VALID'\n",
    "        )\n",
    "        if not activation_fn:\n",
    "            deconv = tf.nn.relu(deconv)\n",
    "            deconv = tf.contrib.layers.batch_norm(\n",
    "                deconv, center=True, scale=True,  decay=0.9,\n",
    "                is_training=is_train, updates_collections=None\n",
    "            )\n",
    "        else:\n",
    "            deconv = activation_fn(deconv)\n",
    "        return deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv2d_new(input_, output_shape,\n",
    "             k_h=4, k_w=4, d_h=2, d_w=2, stddev=0.02,\n",
    "             name=\"deconv2d\", with_w=False, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        # filter : [height, width, output_channels, in_channels]\n",
    "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        \n",
    "        try:\n",
    "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n",
    "                                strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        # Support for versions of TensorFlow before 0.7.0\n",
    "        except AttributeError:\n",
    "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
    "                                strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "        if not activation_fn:\n",
    "            deconv = tf.nn.relu(deconv)\n",
    "            deconv = tf.contrib.layers.batch_norm(\n",
    "                deconv, center=True, scale=True,  decay=0.9,\n",
    "                is_training=is_train, updates_collections=None\n",
    "            )\n",
    "        else:\n",
    "            deconv = activation_fn(deconv)\n",
    "        #print 'deconv shape:', deconv.get_shape()\n",
    "        #print 'w shape:', w.get_shape()\n",
    "        if with_w:\n",
    "            return deconv, w, biases\n",
    "        else:\n",
    "            return deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    shape = input_.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                                 tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "            initializer=tf.constant_initializer(bias_start))\n",
    "        if with_w:\n",
    "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
    "        else:\n",
    "            return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_train=True\n",
    "image_shape = [batch_size, 224, 224, 3]\n",
    "def G(z, scope='Generator'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        #log.warn(scope.name)\n",
    "        s = 224\n",
    "        s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n",
    "        print \"s16 \", s16\n",
    "        gf_dim = 64\n",
    "        c_dim = 3\n",
    "        print z.shape\n",
    "        z_1, h0_w, h0_b = linear(z, gf_dim*8*s16*s16, scope=scope, with_w=True)\n",
    "        z_1 = tf.reshape(z_1, [-1, s16, s16, gf_dim * 8])\n",
    "        z_1 = tf.nn.relu(z_1)\n",
    "        z_1 = tf.contrib.layers.batch_norm(\n",
    "                z_1, center=True, scale=True,  decay=0.9,\n",
    "                is_training=is_train, updates_collections=None\n",
    "            )\n",
    "        print z_1.shape\n",
    "        #z = tf.reshape(z, [batch_size, 1, 1, -1])\n",
    "        #g_1 = deconv2d(z, deconv_info[0], is_train, name='g_1_deconv')\n",
    "        g_1 = deconv2d_new(z_1, [batch_size, s8, s8, gf_dim*4], name='g_1_deconv', with_w=False)\n",
    "        print g_1.shape\n",
    "        #log.info('{} {}'.format(scope.name, g_1))\n",
    "        #g_2 = deconv2d(g_1, deconv_info[1], is_train, name='g_2_deconv')\n",
    "        g_2 = deconv2d_new(g_1, [batch_size, s4, s4, gf_dim*2], name='g_2_deconv', with_w=False)\n",
    "        print g_2.shape\n",
    "        #log.info('{} {}'.format(scope.name, g_2))\n",
    "        #g_3 = deconv2d(g_2, deconv_info[2], is_train, name='g_3_deconv')\n",
    "        g_3 = deconv2d_new(g_2, [batch_size, s2, s2, gf_dim*1], name='g_3_deconv', with_w=False)\n",
    "        print g_3.shape\n",
    "        #log.info('{} {}'.format(scope.name, g_3))\n",
    "        #g_4 = deconv2d(g_3, deconv_info[3], is_train, name='g_4_deconv', activation_fn=tf.tanh)\n",
    "        g_4 = deconv2d_new(g_3, [batch_size, s, s, c_dim], name='g_h4', with_w=False, activation_fn=tf.tanh)\n",
    "        #log.info('{} {}'.format(scope.name, g_4))\n",
    "        print g_4.shape\n",
    "        output = g_4\n",
    "        #print X.get_shape().as_list()\n",
    "        assert output.get_shape().as_list() == image_shape, output.get_shape().as_list()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def D(img, scope='Discriminator', reuse=True):\n",
    "    with tf.variable_scope(scope, reuse=reuse) as scope:\n",
    "        #if not reuse: log.warn(scope.name)\n",
    "        #print img.shape\n",
    "        d_1 = conv2d(img, conv_info[0], is_train, name='d_1_conv')\n",
    "        d_1 = slim.dropout(d_1, keep_prob=0.5, is_training=is_train, scope='d_1_conv/')\n",
    "        #print d_1.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_1))\n",
    "        d_2 = conv2d(d_1, conv_info[1], is_train, name='d_2_conv')\n",
    "        d_2 = slim.dropout(d_2, keep_prob=0.5, is_training=is_train, scope='d_2_conv/')\n",
    "        #print d_2.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_2))\n",
    "        d_3 = conv2d(d_2, conv_info[2], is_train, name='d_3_conv')\n",
    "        d_3 = slim.dropout(d_3, keep_prob=0.5, is_training=is_train, scope='d_3_conv/')\n",
    "        #print d_3.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_3))\n",
    "        d_4 = slim.fully_connected(\n",
    "            tf.reshape(d_3, [batch_size, -1]), n_class, scope='d_4_fc', activation_fn=None)\n",
    "        #print d_4.shape\n",
    "        #if not reuse: log.info('{} {}'.format(scope.name, d_4))\n",
    "        output = d_4\n",
    "        assert output.get_shape().as_list() == [batch_size, n_class]\n",
    "        return tf.nn.softmax(output), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1.0e-6\n",
    "LAMBA = 1\n",
    "# y has dim batch_size x num_classes\n",
    "# entropy 1\n",
    "def marginal_entropy(y):\n",
    "    y_1 = tf.reduce_mean(y, axis=0) #1/N sum y_i\n",
    "    y_2 = -y_1 * tf.log(y_1+epsilon)\n",
    "    y_3 = tf.reduce_sum(y_2)\n",
    "    return y_3\n",
    "\n",
    "def entropy(y):\n",
    "    #batch_size= K.int_shape(y)[0]\n",
    "    y_1 = -y * tf.log(y+epsilon)\n",
    "    y_2 = tf.reduce_sum(y_1,axis=1)\n",
    "    y_3 = tf.reduce_mean(y_2,axis=0)\n",
    "    return y_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500   6   2]\n"
     ]
    }
   ],
   "source": [
    "print deconv_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#list of np arrays\n",
    "def average_grads(grads):\n",
    "    #print type(grads)\n",
    "    #print len(grads)\n",
    "    #print type(grads[0])\n",
    "    #print len(grads[0])\n",
    "    #print type(grads[0][0])\n",
    "    grads_list = []\n",
    "    for grad in grads:\n",
    "        grads_list.append(tf.reduce_mean(grad[0]))\n",
    "    #print \"from average grads\"\n",
    "    #print type(grads_list[0])\n",
    "    return grads_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s16  14\n",
      "(5, 500)\n",
      "(5, 14, 14, 512)\n",
      "(5, 28, 28, 256)\n",
      "(5, 56, 56, 128)\n",
      "(5, 112, 112, 64)\n",
      "(5, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#from util import log\n",
    "#z = tf.random_uniform([batch_size, nz], minval=-1, maxval=1, dtype=tf.float32)\n",
    "z = tf.random_normal([batch_size, nz], dtype=tf.float32)\n",
    "G_sample = G(z)\n",
    "\n",
    "D_real, D_real_logits = D(X, scope='Discriminator', reuse=False)\n",
    "D_fake, D_fake_logits = D(G_sample, scope='Discriminator', reuse=True)\n",
    "\n",
    "D_target = 1./mb_size\n",
    "G_target = 1./(mb_size*2)\n",
    "\n",
    "#Z = tf.reduce_sum(tf.exp(-D_real)) + tf.reduce_sum(tf.exp(-D_fake))\n",
    "\n",
    "#D_loss = tf.reduce_sum(D_target * D_real) + log(Z)\n",
    "#G_loss = tf.reduce_sum(G_target * D_real) + tf.reduce_sum(G_target * D_fake) + log(Z)\n",
    "\n",
    "D_loss = -marginal_entropy(D_real) + entropy(D_real) - entropy(D_fake) + cross_entropy_term *tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=D_real_logits))\n",
    "G_loss = -marginal_entropy(D_fake) + entropy(D_fake)\n",
    "\n",
    "all_vars = tf.trainable_variables()\n",
    "\n",
    "theta_D = [v for v in all_vars if v.name.startswith('Discriminator')]\n",
    "#log.warn(\"********* d_var ********** \"); slim.model_analyzer.analyze_vars(d_var, print_info=True)\n",
    "\n",
    "theta_G = [v for v in all_vars if v.name.startswith(('Generator'))]\n",
    "#log.warn(\"********* g_var ********** \"); slim.model_analyzer.analyze_vars(g_var, print_info=True)\n",
    "\n",
    "D_grad_overall = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .compute_gradients(D_loss, var_list=theta_D, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N))\n",
    "#D_grad = average_grads(D_grad_overall)\n",
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "G_grad_overall = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "             .compute_gradients(G_loss, var_list=theta_G, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N))\n",
    "#G_grad = average_grads(G_grad_overall)\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=theta_G))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess = tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "\n",
    "dir_name ='out_20/'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; D_loss: 0.1401; G_loss: -0.205\n"
     ]
    }
   ],
   "source": [
    "#100,000 is good\n",
    "#print every 1000\n",
    "g_loss_overall = []\n",
    "d_loss_overall = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    N_train = X_train.shape[0]\n",
    "    n_batches = int(np.ceil(N_train / float(batch_size)))\n",
    "    for it in range(num_epochs):\n",
    "        for i, b in enumerate(range(0, N_train, batch_size)):\n",
    "        #for i in range(1):\n",
    "            X_batch = X_train[b : b + batch_size, :]\n",
    "            Y_batch = Y_train[b : b + batch_size, :]\n",
    "            if (X_batch.shape[0] != batch_size): continue\n",
    "#            X_batch = X_train[i : i + batch_size, :]\n",
    "#             Y_batch = Y_train[i : i + batch_size, :]\n",
    "\n",
    "            _, D_loss_curr = sess.run(\n",
    "                [D_solver, D_loss], feed_dict={X: X_batch, y: Y_batch}\n",
    "            )\n",
    "\n",
    "            _, G_loss_curr = sess.run(\n",
    "                [G_solver, G_loss], feed_dict={X: X_batch, y: Y_batch}\n",
    "            )\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "                  .format(i, D_loss_curr, G_loss_curr))\n",
    "                D_grad_cur_overall = sess.run(D_grad_overall, feed_dict={X: X_batch, y: Y_batch})\n",
    "                D_grad_curr = average_grads(D_grad_cur_overall)\n",
    "                D_grad_eval = []\n",
    "                for grad in D_grad_curr:\n",
    "                    D_grad_eval.append(grad.eval(session=sess))\n",
    "                #print D_grad_eval\n",
    "                g_loss_overall.append(G_loss_curr)\n",
    "                d_loss_overall.append(D_loss_curr)\n",
    "                samples = sess.run(G_sample)\n",
    "                truncated_samples = samples[0:16,:]\n",
    "                fig = plot(truncated_samples)\n",
    "                plt.savefig(dir_name+'{}.png'\n",
    "                    .format(str(counter).zfill(3)), bbox_inches='tight')\n",
    "                counter += 1\n",
    "                plt.close(fig)\n",
    "        # Reshuffle\n",
    "        idxs = range(X_train.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        X_train = X_train[idxs]\n",
    "        Y_train = Y_train[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#print g_loss_overall\n",
    "#print d_loss_overall\n",
    "g_loss_plot, = plt.plot(g_loss_overall, label='G Loss')\n",
    "d_loss_plot, = plt.plot(d_loss_overall, label='D Loss')\n",
    "\n",
    "plt.legend([g_loss_plot, d_loss_plot], ['G Loss', 'D Loss'])\n",
    "plt.savefig(dir_name+'plot_for_catgan_.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
